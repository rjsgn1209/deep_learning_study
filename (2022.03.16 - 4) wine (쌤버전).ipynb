{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89cf913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a36635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba2e247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3      4     5     6       7     8     9    10  11  12\n",
       "0   7.4  0.70  0.00  1.9  0.076  11.0  34.0  0.9978  3.51  0.56  9.4   5   1\n",
       "1   7.8  0.88  0.00  2.6  0.098  25.0  67.0  0.9968  3.20  0.68  9.8   5   1\n",
       "2   7.8  0.76  0.04  2.3  0.092  15.0  54.0  0.9970  3.26  0.65  9.8   5   1\n",
       "3  11.2  0.28  0.56  1.9  0.075  17.0  60.0  0.9980  3.16  0.58  9.8   6   1\n",
       "4   7.4  0.70  0.00  1.9  0.076  11.0  34.0  0.9978  3.51  0.56  9.4   5   1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./Data/wine.csv', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "845c7a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6497 entries, 0 to 6496\n",
      "Data columns (total 13 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       6497 non-null   float64\n",
      " 1   1       6497 non-null   float64\n",
      " 2   2       6497 non-null   float64\n",
      " 3   3       6497 non-null   float64\n",
      " 4   4       6497 non-null   float64\n",
      " 5   5       6497 non-null   float64\n",
      " 6   6       6497 non-null   float64\n",
      " 7   7       6497 non-null   float64\n",
      " 8   8       6497 non-null   float64\n",
      " 9   9       6497 non-null   float64\n",
      " 10  10      6497 non-null   float64\n",
      " 11  11      6497 non-null   int64  \n",
      " 12  12      6497 non-null   int64  \n",
      "dtypes: float64(11), int64(2)\n",
      "memory usage: 660.0 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95b73ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "6492    0\n",
       "6493    0\n",
       "6494    0\n",
       "6495    0\n",
       "6496    0\n",
       "Name: 12, Length: 6497, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5933e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48bb932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary classification, muliti classification 두 가지 방식으로 nural network model을 만들고 \n",
    "# train data로 학습시킨 후 test data로 accuracy를 평가하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15ea5140",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.iloc[:, 12].values\n",
    "x = df.iloc[:, :12].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e38c8797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ee6a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_oh = tf.keras.utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d32499d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f24320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62a70b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c98cbeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90c9764a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6497, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d13070cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 36)                468       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 18)                666       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 9)                 171       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 10        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,315\n",
      "Trainable params: 1,315\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(36, activation='relu', input_dim=12))\n",
    "model.add(Dense(18, activation='relu'))\n",
    "model.add(Dense(9, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2e2fe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3674f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4872, 12)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d69eca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4872,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bbd1474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     0\n",
       "10    0\n",
       "11    0\n",
       "12    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfef521d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "244/244 [==============================] - 1s 779us/step - loss: 0.2449 - accuracy: 0.9218\n",
      "Epoch 2/50\n",
      "244/244 [==============================] - 0s 745us/step - loss: 0.1816 - accuracy: 0.9380\n",
      "Epoch 3/50\n",
      "244/244 [==============================] - 0s 740us/step - loss: 0.1508 - accuracy: 0.9489\n",
      "Epoch 4/50\n",
      "244/244 [==============================] - 0s 829us/step - loss: 0.1428 - accuracy: 0.9509\n",
      "Epoch 5/50\n",
      "244/244 [==============================] - 0s 679us/step - loss: 0.1298 - accuracy: 0.9548\n",
      "Epoch 6/50\n",
      "244/244 [==============================] - 0s 706us/step - loss: 0.1168 - accuracy: 0.9594\n",
      "Epoch 7/50\n",
      "244/244 [==============================] - 0s 694us/step - loss: 0.1094 - accuracy: 0.9635\n",
      "Epoch 8/50\n",
      "244/244 [==============================] - 0s 708us/step - loss: 0.1034 - accuracy: 0.9676\n",
      "Epoch 9/50\n",
      "244/244 [==============================] - 0s 683us/step - loss: 0.0953 - accuracy: 0.9688\n",
      "Epoch 10/50\n",
      "244/244 [==============================] - 0s 678us/step - loss: 0.0934 - accuracy: 0.9700\n",
      "Epoch 11/50\n",
      "244/244 [==============================] - 0s 686us/step - loss: 0.0907 - accuracy: 0.9717\n",
      "Epoch 12/50\n",
      "244/244 [==============================] - 0s 675us/step - loss: 0.0898 - accuracy: 0.9690\n",
      "Epoch 13/50\n",
      "244/244 [==============================] - 0s 693us/step - loss: 0.0835 - accuracy: 0.9727\n",
      "Epoch 14/50\n",
      "244/244 [==============================] - 0s 693us/step - loss: 0.0850 - accuracy: 0.9743\n",
      "Epoch 15/50\n",
      "244/244 [==============================] - 0s 716us/step - loss: 0.0852 - accuracy: 0.9723\n",
      "Epoch 16/50\n",
      "244/244 [==============================] - 0s 685us/step - loss: 0.0733 - accuracy: 0.9776\n",
      "Epoch 17/50\n",
      "244/244 [==============================] - 0s 677us/step - loss: 0.0759 - accuracy: 0.9750\n",
      "Epoch 18/50\n",
      "244/244 [==============================] - 0s 676us/step - loss: 0.0753 - accuracy: 0.9758\n",
      "Epoch 19/50\n",
      "244/244 [==============================] - 0s 719us/step - loss: 0.0681 - accuracy: 0.9793\n",
      "Epoch 20/50\n",
      "244/244 [==============================] - 0s 713us/step - loss: 0.0701 - accuracy: 0.9793\n",
      "Epoch 21/50\n",
      "244/244 [==============================] - 0s 720us/step - loss: 0.0663 - accuracy: 0.9791\n",
      "Epoch 22/50\n",
      "244/244 [==============================] - 0s 690us/step - loss: 0.0660 - accuracy: 0.9801\n",
      "Epoch 23/50\n",
      "244/244 [==============================] - 0s 699us/step - loss: 0.0679 - accuracy: 0.9807\n",
      "Epoch 24/50\n",
      "244/244 [==============================] - 0s 871us/step - loss: 0.0678 - accuracy: 0.9778\n",
      "Epoch 25/50\n",
      "244/244 [==============================] - 0s 829us/step - loss: 0.0589 - accuracy: 0.9834\n",
      "Epoch 26/50\n",
      "244/244 [==============================] - 0s 839us/step - loss: 0.0727 - accuracy: 0.9762\n",
      "Epoch 27/50\n",
      "244/244 [==============================] - 0s 697us/step - loss: 0.0600 - accuracy: 0.9811\n",
      "Epoch 28/50\n",
      "244/244 [==============================] - 0s 673us/step - loss: 0.0656 - accuracy: 0.9795\n",
      "Epoch 29/50\n",
      "244/244 [==============================] - 0s 673us/step - loss: 0.0622 - accuracy: 0.9809\n",
      "Epoch 30/50\n",
      "244/244 [==============================] - 0s 788us/step - loss: 0.0696 - accuracy: 0.9772\n",
      "Epoch 31/50\n",
      "244/244 [==============================] - 0s 772us/step - loss: 0.0581 - accuracy: 0.9826\n",
      "Epoch 32/50\n",
      "244/244 [==============================] - 0s 730us/step - loss: 0.0591 - accuracy: 0.9823\n",
      "Epoch 33/50\n",
      "244/244 [==============================] - 0s 713us/step - loss: 0.0570 - accuracy: 0.9832\n",
      "Epoch 34/50\n",
      "244/244 [==============================] - 0s 701us/step - loss: 0.0618 - accuracy: 0.9805\n",
      "Epoch 35/50\n",
      "244/244 [==============================] - 0s 691us/step - loss: 0.0580 - accuracy: 0.9836\n",
      "Epoch 36/50\n",
      "244/244 [==============================] - 0s 734us/step - loss: 0.0605 - accuracy: 0.9828\n",
      "Epoch 37/50\n",
      "244/244 [==============================] - 0s 742us/step - loss: 0.0522 - accuracy: 0.9838\n",
      "Epoch 38/50\n",
      "244/244 [==============================] - 0s 727us/step - loss: 0.0528 - accuracy: 0.9852\n",
      "Epoch 39/50\n",
      "244/244 [==============================] - 0s 717us/step - loss: 0.0566 - accuracy: 0.9836\n",
      "Epoch 40/50\n",
      "244/244 [==============================] - 0s 679us/step - loss: 0.0613 - accuracy: 0.9811\n",
      "Epoch 41/50\n",
      "244/244 [==============================] - 0s 736us/step - loss: 0.0550 - accuracy: 0.9821\n",
      "Epoch 42/50\n",
      "244/244 [==============================] - 0s 754us/step - loss: 0.0548 - accuracy: 0.9840\n",
      "Epoch 43/50\n",
      "244/244 [==============================] - 0s 764us/step - loss: 0.0531 - accuracy: 0.9838\n",
      "Epoch 44/50\n",
      "244/244 [==============================] - 0s 723us/step - loss: 0.0529 - accuracy: 0.9862\n",
      "Epoch 45/50\n",
      "244/244 [==============================] - 0s 730us/step - loss: 0.0611 - accuracy: 0.9823\n",
      "Epoch 46/50\n",
      "244/244 [==============================] - 0s 702us/step - loss: 0.0476 - accuracy: 0.9869\n",
      "Epoch 47/50\n",
      "244/244 [==============================] - 0s 776us/step - loss: 0.0586 - accuracy: 0.9842\n",
      "Epoch 48/50\n",
      "244/244 [==============================] - 0s 712us/step - loss: 0.0516 - accuracy: 0.9848\n",
      "Epoch 49/50\n",
      "244/244 [==============================] - 0s 732us/step - loss: 0.0488 - accuracy: 0.9844\n",
      "Epoch 50/50\n",
      "244/244 [==============================] - 0s 728us/step - loss: 0.0503 - accuracy: 0.9850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b9d0c89b20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train.astype(float), y_train, epochs=50, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b959b8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 0s 698us/step - loss: 0.0595 - accuracy: 0.9822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.059461187571287155, 0.9821538329124451]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d114d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ef595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d43c8edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b405f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 36)                468       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 18)                666       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 9)                 171       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 20        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,325\n",
      "Trainable params: 1,325\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(36, activation='relu', input_dim=12))\n",
    "model.add(Dense(18, activation='relu'))\n",
    "model.add(Dense(9, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71640536",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6504d6f9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "244/244 [==============================] - 0s 743us/step - loss: 0.1968 - accuracy: 0.9358\n",
      "Epoch 2/50\n",
      "244/244 [==============================] - 0s 729us/step - loss: 0.1654 - accuracy: 0.9423\n",
      "Epoch 3/50\n",
      "244/244 [==============================] - 0s 749us/step - loss: 0.1517 - accuracy: 0.9468\n",
      "Epoch 4/50\n",
      "244/244 [==============================] - 0s 748us/step - loss: 0.1386 - accuracy: 0.9530\n",
      "Epoch 5/50\n",
      "244/244 [==============================] - 0s 730us/step - loss: 0.1284 - accuracy: 0.9563\n",
      "Epoch 6/50\n",
      "244/244 [==============================] - 0s 725us/step - loss: 0.1132 - accuracy: 0.9612\n",
      "Epoch 7/50\n",
      "244/244 [==============================] - 0s 730us/step - loss: 0.1082 - accuracy: 0.9649\n",
      "Epoch 8/50\n",
      "244/244 [==============================] - 0s 721us/step - loss: 0.0924 - accuracy: 0.9692\n",
      "Epoch 9/50\n",
      "244/244 [==============================] - 0s 720us/step - loss: 0.0928 - accuracy: 0.9692\n",
      "Epoch 10/50\n",
      "244/244 [==============================] - 0s 723us/step - loss: 0.0754 - accuracy: 0.9760\n",
      "Epoch 11/50\n",
      "244/244 [==============================] - 0s 715us/step - loss: 0.0783 - accuracy: 0.9756\n",
      "Epoch 12/50\n",
      "244/244 [==============================] - 0s 725us/step - loss: 0.0816 - accuracy: 0.9745\n",
      "Epoch 13/50\n",
      "244/244 [==============================] - 0s 734us/step - loss: 0.0656 - accuracy: 0.9803\n",
      "Epoch 14/50\n",
      "244/244 [==============================] - 0s 726us/step - loss: 0.0760 - accuracy: 0.9764\n",
      "Epoch 15/50\n",
      "244/244 [==============================] - 0s 719us/step - loss: 0.0723 - accuracy: 0.9762\n",
      "Epoch 16/50\n",
      "244/244 [==============================] - 0s 754us/step - loss: 0.0722 - accuracy: 0.9768\n",
      "Epoch 17/50\n",
      "244/244 [==============================] - 0s 719us/step - loss: 0.0643 - accuracy: 0.9813\n",
      "Epoch 18/50\n",
      "244/244 [==============================] - 0s 717us/step - loss: 0.0592 - accuracy: 0.9828\n",
      "Epoch 19/50\n",
      "244/244 [==============================] - 0s 729us/step - loss: 0.0612 - accuracy: 0.9809\n",
      "Epoch 20/50\n",
      "244/244 [==============================] - 0s 729us/step - loss: 0.0674 - accuracy: 0.9791\n",
      "Epoch 21/50\n",
      "244/244 [==============================] - 0s 722us/step - loss: 0.0572 - accuracy: 0.9821\n",
      "Epoch 22/50\n",
      "244/244 [==============================] - 0s 729us/step - loss: 0.0606 - accuracy: 0.9826\n",
      "Epoch 23/50\n",
      "244/244 [==============================] - 0s 715us/step - loss: 0.0591 - accuracy: 0.9834\n",
      "Epoch 24/50\n",
      "244/244 [==============================] - 0s 708us/step - loss: 0.0611 - accuracy: 0.9809\n",
      "Epoch 25/50\n",
      "244/244 [==============================] - 0s 723us/step - loss: 0.0563 - accuracy: 0.9830\n",
      "Epoch 26/50\n",
      "244/244 [==============================] - 0s 825us/step - loss: 0.0612 - accuracy: 0.9793\n",
      "Epoch 27/50\n",
      "244/244 [==============================] - 0s 684us/step - loss: 0.0596 - accuracy: 0.9821\n",
      "Epoch 28/50\n",
      "244/244 [==============================] - 0s 720us/step - loss: 0.0564 - accuracy: 0.9838\n",
      "Epoch 29/50\n",
      "244/244 [==============================] - 0s 722us/step - loss: 0.0531 - accuracy: 0.9846\n",
      "Epoch 30/50\n",
      "244/244 [==============================] - 0s 723us/step - loss: 0.0523 - accuracy: 0.9840\n",
      "Epoch 31/50\n",
      "244/244 [==============================] - 0s 733us/step - loss: 0.0607 - accuracy: 0.9801\n",
      "Epoch 32/50\n",
      "244/244 [==============================] - 0s 741us/step - loss: 0.0527 - accuracy: 0.9834\n",
      "Epoch 33/50\n",
      "244/244 [==============================] - 0s 728us/step - loss: 0.0492 - accuracy: 0.9860\n",
      "Epoch 34/50\n",
      "244/244 [==============================] - 0s 723us/step - loss: 0.0583 - accuracy: 0.9821\n",
      "Epoch 35/50\n",
      "244/244 [==============================] - 0s 729us/step - loss: 0.0487 - accuracy: 0.9865\n",
      "Epoch 36/50\n",
      "244/244 [==============================] - 0s 723us/step - loss: 0.0586 - accuracy: 0.9813\n",
      "Epoch 37/50\n",
      "244/244 [==============================] - 0s 717us/step - loss: 0.0515 - accuracy: 0.9848\n",
      "Epoch 38/50\n",
      "244/244 [==============================] - 0s 717us/step - loss: 0.0477 - accuracy: 0.9862\n",
      "Epoch 39/50\n",
      "244/244 [==============================] - 0s 713us/step - loss: 0.0549 - accuracy: 0.9828\n",
      "Epoch 40/50\n",
      "244/244 [==============================] - 0s 712us/step - loss: 0.0484 - accuracy: 0.9852\n",
      "Epoch 41/50\n",
      "244/244 [==============================] - 0s 754us/step - loss: 0.0536 - accuracy: 0.9823\n",
      "Epoch 42/50\n",
      "244/244 [==============================] - 0s 712us/step - loss: 0.0478 - accuracy: 0.9860\n",
      "Epoch 43/50\n",
      "244/244 [==============================] - 0s 728us/step - loss: 0.0454 - accuracy: 0.9871\n",
      "Epoch 44/50\n",
      "244/244 [==============================] - 0s 728us/step - loss: 0.0476 - accuracy: 0.9871\n",
      "Epoch 45/50\n",
      "244/244 [==============================] - 0s 717us/step - loss: 0.0442 - accuracy: 0.9877\n",
      "Epoch 46/50\n",
      "244/244 [==============================] - 0s 705us/step - loss: 0.0465 - accuracy: 0.9854\n",
      "Epoch 47/50\n",
      "244/244 [==============================] - 0s 722us/step - loss: 0.0496 - accuracy: 0.9858\n",
      "Epoch 48/50\n",
      "244/244 [==============================] - 0s 725us/step - loss: 0.0471 - accuracy: 0.9848\n",
      "Epoch 49/50\n",
      "244/244 [==============================] - 0s 736us/step - loss: 0.0490 - accuracy: 0.9856\n",
      "Epoch 50/50\n",
      "244/244 [==============================] - 0s 720us/step - loss: 0.0405 - accuracy: 0.9893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x188ff8faeb0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train.astype(float), y_train, epochs=50, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d7abb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 0s 619us/step - loss: 0.0824 - accuracy: 0.9711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08240143209695816, 0.9710769057273865]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70d27b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./deep_model/wine_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd04ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6299d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model checkpointer\n",
    "# 가장 좋은 결과를 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0f617c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y_oh)\n",
    "model = Sequential()\n",
    "model.add(Dense(36, activation='relu', input_dim=12))\n",
    "model.add(Dense(18, activation='relu'))\n",
    "model.add(Dense(9, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# model.summary()\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "251dff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a4ae78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './deep_model/model_check'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fa0dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62d48a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = './deep_model/model_check/{epoch:02d}-{val_loss:4f}.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath = modelpath, monitor='val_loss', verbose=1, \\\n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2124e008",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 4.6073 - accuracy: 0.5904  \n",
      "Epoch 1: val_loss improved from inf to 0.26128, saving model to ./deep_model/model_check\\01-0.261281.hdf5\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 4.0027 - accuracy: 0.6343 - val_loss: 0.2613 - val_accuracy: 0.9241\n",
      "Epoch 2/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.2419 - accuracy: 0.9173\n",
      "Epoch 2: val_loss improved from 0.26128 to 0.23594, saving model to ./deep_model/model_check\\02-0.235940.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.2323 - accuracy: 0.9220 - val_loss: 0.2359 - val_accuracy: 0.9292\n",
      "Epoch 3/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.2162 - accuracy: 0.9242\n",
      "Epoch 3: val_loss improved from 0.23594 to 0.21614, saving model to ./deep_model/model_check\\03-0.216141.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.2195 - accuracy: 0.9243 - val_loss: 0.2161 - val_accuracy: 0.9333\n",
      "Epoch 4/100\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.2114 - accuracy: 0.9303\n",
      "Epoch 4: val_loss improved from 0.21614 to 0.20970, saving model to ./deep_model/model_check\\04-0.209700.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.2123 - accuracy: 0.9297 - val_loss: 0.2097 - val_accuracy: 0.9364\n",
      "Epoch 5/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.2104 - accuracy: 0.9287\n",
      "Epoch 5: val_loss improved from 0.20970 to 0.20384, saving model to ./deep_model/model_check\\05-0.203837.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.2085 - accuracy: 0.9302 - val_loss: 0.2038 - val_accuracy: 0.9364\n",
      "Epoch 6/100\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 0.2005 - accuracy: 0.9322\n",
      "Epoch 6: val_loss improved from 0.20384 to 0.19814, saving model to ./deep_model/model_check\\06-0.198136.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.2001 - accuracy: 0.9317 - val_loss: 0.1981 - val_accuracy: 0.9364\n",
      "Epoch 7/100\n",
      "61/78 [======================>.......] - ETA: 0s - loss: 0.1856 - accuracy: 0.9357\n",
      "Epoch 7: val_loss improved from 0.19814 to 0.19011, saving model to ./deep_model/model_check\\07-0.190109.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1925 - accuracy: 0.9353 - val_loss: 0.1901 - val_accuracy: 0.9415\n",
      "Epoch 8/100\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.1910 - accuracy: 0.9369\n",
      "Epoch 8: val_loss improved from 0.19011 to 0.18562, saving model to ./deep_model/model_check\\08-0.185616.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1877 - accuracy: 0.9379 - val_loss: 0.1856 - val_accuracy: 0.9415\n",
      "Epoch 9/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.1837 - accuracy: 0.9364\n",
      "Epoch 9: val_loss improved from 0.18562 to 0.17544, saving model to ./deep_model/model_check\\09-0.175444.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1793 - accuracy: 0.9389 - val_loss: 0.1754 - val_accuracy: 0.9405\n",
      "Epoch 10/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.1714 - accuracy: 0.9417\n",
      "Epoch 10: val_loss improved from 0.17544 to 0.17163, saving model to ./deep_model/model_check\\10-0.171627.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1695 - accuracy: 0.9410 - val_loss: 0.1716 - val_accuracy: 0.9415\n",
      "Epoch 11/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.1615 - accuracy: 0.9421\n",
      "Epoch 11: val_loss improved from 0.17163 to 0.16215, saving model to ./deep_model/model_check\\11-0.162154.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1629 - accuracy: 0.9420 - val_loss: 0.1622 - val_accuracy: 0.9405\n",
      "Epoch 12/100\n",
      "60/78 [======================>.......] - ETA: 0s - loss: 0.1607 - accuracy: 0.9433\n",
      "Epoch 12: val_loss improved from 0.16215 to 0.16113, saving model to ./deep_model/model_check\\12-0.161132.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.9448 - val_loss: 0.1611 - val_accuracy: 0.9426\n",
      "Epoch 13/100\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.1420 - accuracy: 0.9521\n",
      "Epoch 13: val_loss improved from 0.16113 to 0.15471, saving model to ./deep_model/model_check\\13-0.154707.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.9494 - val_loss: 0.1547 - val_accuracy: 0.9436\n",
      "Epoch 14/100\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.1376 - accuracy: 0.9510\n",
      "Epoch 14: val_loss improved from 0.15471 to 0.14251, saving model to ./deep_model/model_check\\14-0.142513.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1398 - accuracy: 0.9507 - val_loss: 0.1425 - val_accuracy: 0.9467\n",
      "Epoch 15/100\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.1358 - accuracy: 0.9499\n",
      "Epoch 15: val_loss improved from 0.14251 to 0.13695, saving model to ./deep_model/model_check\\15-0.136947.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1354 - accuracy: 0.9505 - val_loss: 0.1369 - val_accuracy: 0.9497\n",
      "Epoch 16/100\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.1291 - accuracy: 0.9540\n",
      "Epoch 16: val_loss improved from 0.13695 to 0.13567, saving model to ./deep_model/model_check\\16-0.135670.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9541 - val_loss: 0.1357 - val_accuracy: 0.9477\n",
      "Epoch 17/100\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.1298 - accuracy: 0.9531\n",
      "Epoch 17: val_loss improved from 0.13567 to 0.12852, saving model to ./deep_model/model_check\\17-0.128524.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9548 - val_loss: 0.1285 - val_accuracy: 0.9538\n",
      "Epoch 18/100\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.1175 - accuracy: 0.9555\n",
      "Epoch 18: val_loss did not improve from 0.12852\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1188 - accuracy: 0.9556 - val_loss: 0.1378 - val_accuracy: 0.9477\n",
      "Epoch 19/100\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.1141 - accuracy: 0.9572\n",
      "Epoch 19: val_loss improved from 0.12852 to 0.12277, saving model to ./deep_model/model_check\\19-0.122769.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1133 - accuracy: 0.9571 - val_loss: 0.1228 - val_accuracy: 0.9569\n",
      "Epoch 20/100\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.1165 - accuracy: 0.9586\n",
      "Epoch 20: val_loss did not improve from 0.12277\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.9600 - val_loss: 0.1228 - val_accuracy: 0.9559\n",
      "Epoch 21/100\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.1048 - accuracy: 0.9644\n",
      "Epoch 21: val_loss improved from 0.12277 to 0.11716, saving model to ./deep_model/model_check\\21-0.117159.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.9610 - val_loss: 0.1172 - val_accuracy: 0.9610\n",
      "Epoch 22/100\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.1018 - accuracy: 0.9657\n",
      "Epoch 22: val_loss did not improve from 0.11716\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1044 - accuracy: 0.9648 - val_loss: 0.1211 - val_accuracy: 0.9559\n",
      "Epoch 23/100\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0989 - accuracy: 0.9639\n",
      "Epoch 23: val_loss improved from 0.11716 to 0.11106, saving model to ./deep_model/model_check\\23-0.111059.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0984 - accuracy: 0.9636 - val_loss: 0.1111 - val_accuracy: 0.9703\n",
      "Epoch 24/100\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0975 - accuracy: 0.9683\n",
      "Epoch 24: val_loss improved from 0.11106 to 0.10771, saving model to ./deep_model/model_check\\24-0.107713.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.9677 - val_loss: 0.1077 - val_accuracy: 0.9744\n",
      "Epoch 25/100\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0941 - accuracy: 0.9674\n",
      "Epoch 25: val_loss did not improve from 0.10771\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.9669 - val_loss: 0.1155 - val_accuracy: 0.9641\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/78 [============================>.] - ETA: 0s - loss: 0.0928 - accuracy: 0.9692\n",
      "Epoch 26: val_loss did not improve from 0.10771\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.9690 - val_loss: 0.1160 - val_accuracy: 0.9610\n",
      "Epoch 27/100\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0848 - accuracy: 0.9683\n",
      "Epoch 27: val_loss improved from 0.10771 to 0.10236, saving model to ./deep_model/model_check\\27-0.102355.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9692 - val_loss: 0.1024 - val_accuracy: 0.9723\n",
      "Epoch 28/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0819 - accuracy: 0.9739\n",
      "Epoch 28: val_loss improved from 0.10236 to 0.10217, saving model to ./deep_model/model_check\\28-0.102174.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.9728 - val_loss: 0.1022 - val_accuracy: 0.9723\n",
      "Epoch 29/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0856 - accuracy: 0.9675\n",
      "Epoch 29: val_loss did not improve from 0.10217\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9684 - val_loss: 0.1052 - val_accuracy: 0.9723\n",
      "Epoch 30/100\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0808 - accuracy: 0.9718\n",
      "Epoch 30: val_loss did not improve from 0.10217\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.9718 - val_loss: 0.1051 - val_accuracy: 0.9703\n",
      "Epoch 31/100\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0819 - accuracy: 0.9739\n",
      "Epoch 31: val_loss improved from 0.10217 to 0.09790, saving model to ./deep_model/model_check\\31-0.097896.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.9746 - val_loss: 0.0979 - val_accuracy: 0.9733\n",
      "Epoch 32/100\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0833 - accuracy: 0.9744\n",
      "Epoch 32: val_loss did not improve from 0.09790\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0806 - accuracy: 0.9738 - val_loss: 0.1000 - val_accuracy: 0.9672\n",
      "Epoch 33/100\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0777 - accuracy: 0.9732\n",
      "Epoch 33: val_loss improved from 0.09790 to 0.09305, saving model to ./deep_model/model_check\\33-0.093049.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.9746 - val_loss: 0.0930 - val_accuracy: 0.9744\n",
      "Epoch 34/100\n",
      "62/78 [======================>.......] - ETA: 0s - loss: 0.0745 - accuracy: 0.9752\n",
      "Epoch 34: val_loss improved from 0.09305 to 0.09091, saving model to ./deep_model/model_check\\34-0.090912.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0748 - accuracy: 0.9759 - val_loss: 0.0909 - val_accuracy: 0.9754\n",
      "Epoch 35/100\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0737 - accuracy: 0.9755\n",
      "Epoch 35: val_loss improved from 0.09091 to 0.09017, saving model to ./deep_model/model_check\\35-0.090174.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0713 - accuracy: 0.9766 - val_loss: 0.0902 - val_accuracy: 0.9764\n",
      "Epoch 36/100\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0737 - accuracy: 0.9757\n",
      "Epoch 36: val_loss improved from 0.09017 to 0.08670, saving model to ./deep_model/model_check\\36-0.086696.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0779 - accuracy: 0.9746 - val_loss: 0.0867 - val_accuracy: 0.9733\n",
      "Epoch 37/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0704 - accuracy: 0.9777\n",
      "Epoch 37: val_loss improved from 0.08670 to 0.08599, saving model to ./deep_model/model_check\\37-0.085986.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0733 - accuracy: 0.9769 - val_loss: 0.0860 - val_accuracy: 0.9774\n",
      "Epoch 38/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0618 - accuracy: 0.9806\n",
      "Epoch 38: val_loss improved from 0.08599 to 0.08506, saving model to ./deep_model/model_check\\38-0.085058.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 0.9787 - val_loss: 0.0851 - val_accuracy: 0.9774\n",
      "Epoch 39/100\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0644 - accuracy: 0.9803\n",
      "Epoch 39: val_loss did not improve from 0.08506\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0654 - accuracy: 0.9802 - val_loss: 0.0979 - val_accuracy: 0.9713\n",
      "Epoch 40/100\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0682 - accuracy: 0.9797\n",
      "Epoch 40: val_loss did not improve from 0.08506\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0643 - accuracy: 0.9802 - val_loss: 0.0890 - val_accuracy: 0.9774\n",
      "Epoch 41/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0605 - accuracy: 0.9820\n",
      "Epoch 41: val_loss did not improve from 0.08506\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0641 - accuracy: 0.9808 - val_loss: 0.1168 - val_accuracy: 0.9662\n",
      "Epoch 42/100\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0671 - accuracy: 0.9769\n",
      "Epoch 42: val_loss improved from 0.08506 to 0.08051, saving model to ./deep_model/model_check\\42-0.080505.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0684 - accuracy: 0.9774 - val_loss: 0.0805 - val_accuracy: 0.9774\n",
      "Epoch 43/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0615 - accuracy: 0.9815\n",
      "Epoch 43: val_loss did not improve from 0.08051\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.9810 - val_loss: 0.1011 - val_accuracy: 0.9754\n",
      "Epoch 44/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0624 - accuracy: 0.9803\n",
      "Epoch 44: val_loss did not improve from 0.08051\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0621 - accuracy: 0.9805 - val_loss: 0.0847 - val_accuracy: 0.9774\n",
      "Epoch 45/100\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0697 - accuracy: 0.9783\n",
      "Epoch 45: val_loss did not improve from 0.08051\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.9792 - val_loss: 0.0808 - val_accuracy: 0.9774\n",
      "Epoch 46/100\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0703 - accuracy: 0.9757\n",
      "Epoch 46: val_loss did not improve from 0.08051\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.9751 - val_loss: 0.0895 - val_accuracy: 0.9744\n",
      "Epoch 47/100\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0652 - accuracy: 0.9806\n",
      "Epoch 47: val_loss did not improve from 0.08051\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0603 - accuracy: 0.9826 - val_loss: 0.0898 - val_accuracy: 0.9764\n",
      "Epoch 48/100\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0619 - accuracy: 0.9803\n",
      "Epoch 48: val_loss improved from 0.08051 to 0.08000, saving model to ./deep_model/model_check\\48-0.079996.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0601 - accuracy: 0.9813 - val_loss: 0.0800 - val_accuracy: 0.9764\n",
      "Epoch 49/100\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0587 - accuracy: 0.9821\n",
      "Epoch 49: val_loss did not improve from 0.08000\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0569 - accuracy: 0.9818 - val_loss: 0.0831 - val_accuracy: 0.9774\n",
      "Epoch 50/100\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0541 - accuracy: 0.9833\n",
      "Epoch 50: val_loss did not improve from 0.08000\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0569 - accuracy: 0.9815 - val_loss: 0.0849 - val_accuracy: 0.9744\n",
      "Epoch 51/100\n",
      "62/78 [======================>.......] - ETA: 0s - loss: 0.0625 - accuracy: 0.9784\n",
      "Epoch 51: val_loss did not improve from 0.08000\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.9779 - val_loss: 0.0875 - val_accuracy: 0.9754\n",
      "Epoch 52/100\n",
      "63/78 [=======================>......] - ETA: 0s - loss: 0.0650 - accuracy: 0.9794\n",
      "Epoch 52: val_loss did not improve from 0.08000\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0643 - accuracy: 0.9792 - val_loss: 0.0938 - val_accuracy: 0.9764\n",
      "Epoch 53/100\n",
      "56/78 [====================>.........] - ETA: 0s - loss: 0.0562 - accuracy: 0.9804\n",
      "Epoch 53: val_loss did not improve from 0.08000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0616 - accuracy: 0.9797 - val_loss: 0.0836 - val_accuracy: 0.9774\n",
      "Epoch 54/100\n",
      "59/78 [=====================>........] - ETA: 0s - loss: 0.0594 - accuracy: 0.9817\n",
      "Epoch 54: val_loss did not improve from 0.08000\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0579 - accuracy: 0.9820 - val_loss: 0.0950 - val_accuracy: 0.9703\n",
      "Epoch 55/100\n",
      "60/78 [======================>.......] - ETA: 0s - loss: 0.0585 - accuracy: 0.9833\n",
      "Epoch 55: val_loss improved from 0.08000 to 0.07965, saving model to ./deep_model/model_check\\55-0.079650.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0585 - accuracy: 0.9826 - val_loss: 0.0797 - val_accuracy: 0.9764\n",
      "Epoch 56/100\n",
      "63/78 [=======================>......] - ETA: 0s - loss: 0.0495 - accuracy: 0.9860\n",
      "Epoch 56: val_loss did not improve from 0.07965\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0529 - accuracy: 0.9843 - val_loss: 0.1032 - val_accuracy: 0.9713\n",
      "Epoch 57/100\n",
      "63/78 [=======================>......] - ETA: 0s - loss: 0.0560 - accuracy: 0.9810\n",
      "Epoch 57: val_loss improved from 0.07965 to 0.07481, saving model to ./deep_model/model_check\\57-0.074813.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0568 - accuracy: 0.9813 - val_loss: 0.0748 - val_accuracy: 0.9805\n",
      "Epoch 58/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0540 - accuracy: 0.9806\n",
      "Epoch 58: val_loss did not improve from 0.07481\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0565 - accuracy: 0.9810 - val_loss: 0.0815 - val_accuracy: 0.9744\n",
      "Epoch 59/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0584 - accuracy: 0.9820\n",
      "Epoch 59: val_loss did not improve from 0.07481\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0568 - accuracy: 0.9831 - val_loss: 0.0871 - val_accuracy: 0.9723\n",
      "Epoch 60/100\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0569 - accuracy: 0.9831\n",
      "Epoch 60: val_loss did not improve from 0.07481\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0570 - accuracy: 0.9831 - val_loss: 0.1097 - val_accuracy: 0.9610\n",
      "Epoch 61/100\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0570 - accuracy: 0.9834\n",
      "Epoch 61: val_loss did not improve from 0.07481\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0588 - accuracy: 0.9828 - val_loss: 0.0851 - val_accuracy: 0.9754\n",
      "Epoch 62/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0595 - accuracy: 0.9817\n",
      "Epoch 62: val_loss did not improve from 0.07481\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0602 - accuracy: 0.9823 - val_loss: 0.0800 - val_accuracy: 0.9805\n",
      "Epoch 63/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0471 - accuracy: 0.9854\n",
      "Epoch 63: val_loss did not improve from 0.07481\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0506 - accuracy: 0.9838 - val_loss: 0.0832 - val_accuracy: 0.9774\n",
      "Epoch 64/100\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0585 - accuracy: 0.9803\n",
      "Epoch 64: val_loss did not improve from 0.07481\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0560 - accuracy: 0.9810 - val_loss: 0.1021 - val_accuracy: 0.9713\n",
      "Epoch 65/100\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0489 - accuracy: 0.9846\n",
      "Epoch 65: val_loss did not improve from 0.07481\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0522 - accuracy: 0.9831 - val_loss: 0.0841 - val_accuracy: 0.9754\n",
      "Epoch 66/100\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0573 - accuracy: 0.9812\n",
      "Epoch 66: val_loss did not improve from 0.07481\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0564 - accuracy: 0.9813 - val_loss: 0.0948 - val_accuracy: 0.9713\n",
      "Epoch 67/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0546 - accuracy: 0.9830\n",
      "Epoch 67: val_loss did not improve from 0.07481\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0526 - accuracy: 0.9836 - val_loss: 0.0823 - val_accuracy: 0.9764\n",
      "Epoch 68/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0568 - accuracy: 0.9814\n",
      "Epoch 68: val_loss did not improve from 0.07481\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0550 - accuracy: 0.9818 - val_loss: 0.0757 - val_accuracy: 0.9785\n",
      "Epoch 69/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0521 - accuracy: 0.9839\n",
      "Epoch 69: val_loss improved from 0.07481 to 0.07202, saving model to ./deep_model/model_check\\69-0.072025.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0516 - accuracy: 0.9838 - val_loss: 0.0720 - val_accuracy: 0.9805\n",
      "Epoch 70/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0480 - accuracy: 0.9849\n",
      "Epoch 70: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0496 - accuracy: 0.9851 - val_loss: 0.0768 - val_accuracy: 0.9785\n",
      "Epoch 71/100\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0524 - accuracy: 0.9830\n",
      "Epoch 71: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0594 - accuracy: 0.9820 - val_loss: 0.0897 - val_accuracy: 0.9733\n",
      "Epoch 72/100\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0520 - accuracy: 0.9835\n",
      "Epoch 72: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0515 - accuracy: 0.9836 - val_loss: 0.0735 - val_accuracy: 0.9805\n",
      "Epoch 73/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0470 - accuracy: 0.9852\n",
      "Epoch 73: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0488 - accuracy: 0.9843 - val_loss: 0.0739 - val_accuracy: 0.9815\n",
      "Epoch 74/100\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0604 - accuracy: 0.9808\n",
      "Epoch 74: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0605 - accuracy: 0.9808 - val_loss: 0.0795 - val_accuracy: 0.9785\n",
      "Epoch 75/100\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0462 - accuracy: 0.9859\n",
      "Epoch 75: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0465 - accuracy: 0.9861 - val_loss: 0.0789 - val_accuracy: 0.9795\n",
      "Epoch 76/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0526 - accuracy: 0.9821\n",
      "Epoch 76: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0499 - accuracy: 0.9828 - val_loss: 0.0791 - val_accuracy: 0.9774\n",
      "Epoch 77/100\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0514 - accuracy: 0.9850\n",
      "Epoch 77: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0495 - accuracy: 0.9859 - val_loss: 0.0873 - val_accuracy: 0.9764\n",
      "Epoch 78/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0546 - accuracy: 0.9814\n",
      "Epoch 78: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0561 - accuracy: 0.9818 - val_loss: 0.0741 - val_accuracy: 0.9795\n",
      "Epoch 79/100\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0558 - accuracy: 0.9816\n",
      "Epoch 79: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0547 - accuracy: 0.9820 - val_loss: 0.0775 - val_accuracy: 0.9764\n",
      "Epoch 80/100\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0433 - accuracy: 0.9866\n",
      "Epoch 80: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0482 - accuracy: 0.9859 - val_loss: 0.0797 - val_accuracy: 0.9795\n",
      "Epoch 81/100\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0487 - accuracy: 0.9842\n",
      "Epoch 81: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0505 - accuracy: 0.9838 - val_loss: 0.0854 - val_accuracy: 0.9754\n",
      "Epoch 82/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0440 - accuracy: 0.9864\n",
      "Epoch 82: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0433 - accuracy: 0.9859 - val_loss: 0.0896 - val_accuracy: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0482 - accuracy: 0.9857\n",
      "Epoch 83: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0474 - accuracy: 0.9859 - val_loss: 0.0822 - val_accuracy: 0.9754\n",
      "Epoch 84/100\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0536 - accuracy: 0.9838\n",
      "Epoch 84: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0544 - accuracy: 0.9833 - val_loss: 0.0799 - val_accuracy: 0.9754\n",
      "Epoch 85/100\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0468 - accuracy: 0.9858\n",
      "Epoch 85: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0461 - accuracy: 0.9859 - val_loss: 0.1954 - val_accuracy: 0.9508\n",
      "Epoch 86/100\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0600 - accuracy: 0.9828\n",
      "Epoch 86: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.9828 - val_loss: 0.0799 - val_accuracy: 0.9826\n",
      "Epoch 87/100\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0477 - accuracy: 0.9851\n",
      "Epoch 87: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0453 - accuracy: 0.9854 - val_loss: 0.0782 - val_accuracy: 0.9774\n",
      "Epoch 88/100\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0498 - accuracy: 0.9830\n",
      "Epoch 88: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0544 - accuracy: 0.9823 - val_loss: 0.0854 - val_accuracy: 0.9764\n",
      "Epoch 89/100\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0489 - accuracy: 0.9855\n",
      "Epoch 89: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0479 - accuracy: 0.9859 - val_loss: 0.0735 - val_accuracy: 0.9785\n",
      "Epoch 90/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0523 - accuracy: 0.9832\n",
      "Epoch 90: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0535 - accuracy: 0.9823 - val_loss: 0.1023 - val_accuracy: 0.9672\n",
      "Epoch 91/100\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0407 - accuracy: 0.9871\n",
      "Epoch 91: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0460 - accuracy: 0.9859 - val_loss: 0.1022 - val_accuracy: 0.9672\n",
      "Epoch 92/100\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0482 - accuracy: 0.9833\n",
      "Epoch 92: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0466 - accuracy: 0.9841 - val_loss: 0.0888 - val_accuracy: 0.9754\n",
      "Epoch 93/100\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0445 - accuracy: 0.9845\n",
      "Epoch 93: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0462 - accuracy: 0.9843 - val_loss: 0.0954 - val_accuracy: 0.9723\n",
      "Epoch 94/100\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0498 - accuracy: 0.9853\n",
      "Epoch 94: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0475 - accuracy: 0.9859 - val_loss: 0.0880 - val_accuracy: 0.9733\n",
      "Epoch 95/100\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0467 - accuracy: 0.9855\n",
      "Epoch 95: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0455 - accuracy: 0.9861 - val_loss: 0.0833 - val_accuracy: 0.9805\n",
      "Epoch 96/100\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0414 - accuracy: 0.9864\n",
      "Epoch 96: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0424 - accuracy: 0.9867 - val_loss: 0.0751 - val_accuracy: 0.9785\n",
      "Epoch 97/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0401 - accuracy: 0.9858\n",
      "Epoch 97: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0442 - accuracy: 0.9854 - val_loss: 0.0815 - val_accuracy: 0.9795\n",
      "Epoch 98/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0426 - accuracy: 0.9864\n",
      "Epoch 98: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0462 - accuracy: 0.9854 - val_loss: 0.0736 - val_accuracy: 0.9815\n",
      "Epoch 99/100\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0456 - accuracy: 0.9861\n",
      "Epoch 99: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0437 - accuracy: 0.9864 - val_loss: 0.0828 - val_accuracy: 0.9785\n",
      "Epoch 100/100\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0488 - accuracy: 0.9846\n",
      "Epoch 100: val_loss did not improve from 0.07202\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0483 - accuracy: 0.9838 - val_loss: 0.0823 - val_accuracy: 0.9774\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, validation_split=0.2, batch_size=50, epochs=100, \\\n",
    "                    callbacks=[checkpointer])\n",
    "\n",
    "# val_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "faedf089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [4.002722263336182,\n",
       "  0.232268288731575,\n",
       "  0.21950627863407135,\n",
       "  0.2122841626405716,\n",
       "  0.20851540565490723,\n",
       "  0.20009417831897736,\n",
       "  0.19248747825622559,\n",
       "  0.1876549869775772,\n",
       "  0.1792939305305481,\n",
       "  0.1694975048303604,\n",
       "  0.16293767094612122,\n",
       "  0.15430159866809845,\n",
       "  0.1457509547472,\n",
       "  0.13981668651103973,\n",
       "  0.1353950947523117,\n",
       "  0.12819963693618774,\n",
       "  0.1264273226261139,\n",
       "  0.11875902116298676,\n",
       "  0.113273024559021,\n",
       "  0.11405716091394424,\n",
       "  0.11001941561698914,\n",
       "  0.10437413305044174,\n",
       "  0.09844174236059189,\n",
       "  0.09652316570281982,\n",
       "  0.09671405702829361,\n",
       "  0.09247338771820068,\n",
       "  0.08695551007986069,\n",
       "  0.08317450433969498,\n",
       "  0.08853554725646973,\n",
       "  0.08129402995109558,\n",
       "  0.0805099830031395,\n",
       "  0.08063674718141556,\n",
       "  0.07508327811956406,\n",
       "  0.07478011399507523,\n",
       "  0.071299247443676,\n",
       "  0.07792145758867264,\n",
       "  0.07331616431474686,\n",
       "  0.0675165206193924,\n",
       "  0.06541692465543747,\n",
       "  0.06431291252374649,\n",
       "  0.06410380452871323,\n",
       "  0.06838467717170715,\n",
       "  0.0636223703622818,\n",
       "  0.06209674850106239,\n",
       "  0.06698839366436005,\n",
       "  0.07350298017263412,\n",
       "  0.06025583669543266,\n",
       "  0.0600927509367466,\n",
       "  0.05691254138946533,\n",
       "  0.056934136897325516,\n",
       "  0.06524056196212769,\n",
       "  0.0643404871225357,\n",
       "  0.061626773327589035,\n",
       "  0.057878050953149796,\n",
       "  0.058457985520362854,\n",
       "  0.0528515987098217,\n",
       "  0.0568343810737133,\n",
       "  0.05649001896381378,\n",
       "  0.05684104561805725,\n",
       "  0.056982360780239105,\n",
       "  0.05884593725204468,\n",
       "  0.060200002044439316,\n",
       "  0.050582632422447205,\n",
       "  0.05604990944266319,\n",
       "  0.05215160921216011,\n",
       "  0.05637294054031372,\n",
       "  0.052567921578884125,\n",
       "  0.055037084966897964,\n",
       "  0.05156969651579857,\n",
       "  0.04960270971059799,\n",
       "  0.059362974017858505,\n",
       "  0.051494792103767395,\n",
       "  0.04875922575592995,\n",
       "  0.06049828976392746,\n",
       "  0.046461332589387894,\n",
       "  0.049859944730997086,\n",
       "  0.04946320131421089,\n",
       "  0.05605057626962662,\n",
       "  0.054717764258384705,\n",
       "  0.0482211709022522,\n",
       "  0.050512298941612244,\n",
       "  0.043316036462783813,\n",
       "  0.04741533473134041,\n",
       "  0.054401807487010956,\n",
       "  0.046096593141555786,\n",
       "  0.059933990240097046,\n",
       "  0.045256972312927246,\n",
       "  0.05435248464345932,\n",
       "  0.04786287620663643,\n",
       "  0.053518861532211304,\n",
       "  0.04604519158601761,\n",
       "  0.04657364636659622,\n",
       "  0.046188484877347946,\n",
       "  0.04747216776013374,\n",
       "  0.045478712767362595,\n",
       "  0.042433109134435654,\n",
       "  0.04423931613564491,\n",
       "  0.04623128101229668,\n",
       "  0.043694235384464264,\n",
       "  0.04834787920117378],\n",
       " 'accuracy': [0.6343340873718262,\n",
       "  0.9219912886619568,\n",
       "  0.9243007302284241,\n",
       "  0.9296895265579224,\n",
       "  0.9302027225494385,\n",
       "  0.9317423701286316,\n",
       "  0.9353348612785339,\n",
       "  0.937900960445404,\n",
       "  0.9389273524284363,\n",
       "  0.9409802556037903,\n",
       "  0.9420066475868225,\n",
       "  0.9448293447494507,\n",
       "  0.94944828748703,\n",
       "  0.9507313370704651,\n",
       "  0.950474739074707,\n",
       "  0.9540672302246094,\n",
       "  0.9548370838165283,\n",
       "  0.9556068778038025,\n",
       "  0.9571465253829956,\n",
       "  0.9599692225456238,\n",
       "  0.960995614528656,\n",
       "  0.9648447632789612,\n",
       "  0.9635617136955261,\n",
       "  0.9676674604415894,\n",
       "  0.9668976068496704,\n",
       "  0.9689504504203796,\n",
       "  0.9692071080207825,\n",
       "  0.9727995991706848,\n",
       "  0.9684372544288635,\n",
       "  0.9717731475830078,\n",
       "  0.974595844745636,\n",
       "  0.973825991153717,\n",
       "  0.974595844745636,\n",
       "  0.975878894329071,\n",
       "  0.9766486883163452,\n",
       "  0.974595844745636,\n",
       "  0.9769052863121033,\n",
       "  0.9787015914916992,\n",
       "  0.9802412390708923,\n",
       "  0.9802412390708923,\n",
       "  0.9807544350624084,\n",
       "  0.9774185419082642,\n",
       "  0.9810110330581665,\n",
       "  0.9804978370666504,\n",
       "  0.9792147874832153,\n",
       "  0.9751090407371521,\n",
       "  0.9825506806373596,\n",
       "  0.9812676310539246,\n",
       "  0.9817808866500854,\n",
       "  0.9815242290496826,\n",
       "  0.9779317378997803,\n",
       "  0.9792147874832153,\n",
       "  0.9797279834747314,\n",
       "  0.9820374846458435,\n",
       "  0.9825506806373596,\n",
       "  0.9843469262123108,\n",
       "  0.9812676310539246,\n",
       "  0.9810110330581665,\n",
       "  0.9830638766288757,\n",
       "  0.9830638766288757,\n",
       "  0.9828072786331177,\n",
       "  0.9822940826416016,\n",
       "  0.9838337302207947,\n",
       "  0.9810110330581665,\n",
       "  0.9830638766288757,\n",
       "  0.9812676310539246,\n",
       "  0.9835771322250366,\n",
       "  0.9817808866500854,\n",
       "  0.9838337302207947,\n",
       "  0.9851167798042297,\n",
       "  0.9820374846458435,\n",
       "  0.9835771322250366,\n",
       "  0.9843469262123108,\n",
       "  0.9807544350624084,\n",
       "  0.986143171787262,\n",
       "  0.9828072786331177,\n",
       "  0.9858865737915039,\n",
       "  0.9817808866500854,\n",
       "  0.9820374846458435,\n",
       "  0.9858865737915039,\n",
       "  0.9838337302207947,\n",
       "  0.9858865737915039,\n",
       "  0.9858865737915039,\n",
       "  0.9833204746246338,\n",
       "  0.9858865737915039,\n",
       "  0.9828072786331177,\n",
       "  0.9853733777999878,\n",
       "  0.9822940826416016,\n",
       "  0.9858865737915039,\n",
       "  0.9822940826416016,\n",
       "  0.9858865737915039,\n",
       "  0.9840903282165527,\n",
       "  0.9843469262123108,\n",
       "  0.9858865737915039,\n",
       "  0.986143171787262,\n",
       "  0.9866564273834229,\n",
       "  0.9853733777999878,\n",
       "  0.9853733777999878,\n",
       "  0.98639976978302,\n",
       "  0.9838337302207947],\n",
       " 'val_loss': [0.26128122210502625,\n",
       "  0.23593953251838684,\n",
       "  0.21614110469818115,\n",
       "  0.20970016717910767,\n",
       "  0.20383727550506592,\n",
       "  0.19813649356365204,\n",
       "  0.19010941684246063,\n",
       "  0.18561625480651855,\n",
       "  0.17544378340244293,\n",
       "  0.17162710428237915,\n",
       "  0.16215446591377258,\n",
       "  0.16113239526748657,\n",
       "  0.1547066569328308,\n",
       "  0.1425129771232605,\n",
       "  0.13694719970226288,\n",
       "  0.13567019999027252,\n",
       "  0.12852412462234497,\n",
       "  0.13780364394187927,\n",
       "  0.12276865541934967,\n",
       "  0.12280150502920151,\n",
       "  0.11715859919786453,\n",
       "  0.12106399983167648,\n",
       "  0.11105851829051971,\n",
       "  0.10771330446004868,\n",
       "  0.11549052596092224,\n",
       "  0.11603846400976181,\n",
       "  0.10235501080751419,\n",
       "  0.10217376053333282,\n",
       "  0.10515349358320236,\n",
       "  0.10513511300086975,\n",
       "  0.09789561480283737,\n",
       "  0.09999395161867142,\n",
       "  0.09304865449666977,\n",
       "  0.09091192483901978,\n",
       "  0.09017409384250641,\n",
       "  0.08669599145650864,\n",
       "  0.08598627895116806,\n",
       "  0.08505786955356598,\n",
       "  0.09788690507411957,\n",
       "  0.08900967985391617,\n",
       "  0.11675622314214706,\n",
       "  0.08050525933504105,\n",
       "  0.10106056928634644,\n",
       "  0.0847497433423996,\n",
       "  0.08076143264770508,\n",
       "  0.08949755877256393,\n",
       "  0.08982013165950775,\n",
       "  0.07999618351459503,\n",
       "  0.08312705159187317,\n",
       "  0.08488820493221283,\n",
       "  0.0875348225235939,\n",
       "  0.09378331154584885,\n",
       "  0.08359462767839432,\n",
       "  0.094980388879776,\n",
       "  0.07965036481618881,\n",
       "  0.10315179079771042,\n",
       "  0.07481343299150467,\n",
       "  0.08146124333143234,\n",
       "  0.08705577254295349,\n",
       "  0.10967618972063065,\n",
       "  0.0851324200630188,\n",
       "  0.07998210936784744,\n",
       "  0.08315562456846237,\n",
       "  0.10213704407215118,\n",
       "  0.08412070572376251,\n",
       "  0.0947929248213768,\n",
       "  0.08229243010282516,\n",
       "  0.07571125775575638,\n",
       "  0.07202454656362534,\n",
       "  0.07675839960575104,\n",
       "  0.08969686180353165,\n",
       "  0.07346222549676895,\n",
       "  0.07389990985393524,\n",
       "  0.07953134924173355,\n",
       "  0.07890921086072922,\n",
       "  0.07909134775400162,\n",
       "  0.08731360733509064,\n",
       "  0.07409445196390152,\n",
       "  0.07753921300172806,\n",
       "  0.07971249520778656,\n",
       "  0.08539313077926636,\n",
       "  0.08963135629892349,\n",
       "  0.08215554058551788,\n",
       "  0.07989520579576492,\n",
       "  0.19539609551429749,\n",
       "  0.0798802524805069,\n",
       "  0.0782080590724945,\n",
       "  0.08543253690004349,\n",
       "  0.07354605942964554,\n",
       "  0.10233354568481445,\n",
       "  0.1022416278719902,\n",
       "  0.08881394565105438,\n",
       "  0.0953793004155159,\n",
       "  0.08795644342899323,\n",
       "  0.08334044367074966,\n",
       "  0.07514244318008423,\n",
       "  0.08152876049280167,\n",
       "  0.07358752191066742,\n",
       "  0.08280468732118607,\n",
       "  0.08226232975721359],\n",
       " 'val_accuracy': [0.9241025447845459,\n",
       "  0.9292307496070862,\n",
       "  0.9333333373069763,\n",
       "  0.9364102482795715,\n",
       "  0.9364102482795715,\n",
       "  0.9364102482795715,\n",
       "  0.9415384531021118,\n",
       "  0.9415384531021118,\n",
       "  0.9405128359794617,\n",
       "  0.9415384531021118,\n",
       "  0.9405128359794617,\n",
       "  0.9425641298294067,\n",
       "  0.9435897469520569,\n",
       "  0.9466666579246521,\n",
       "  0.9497435688972473,\n",
       "  0.947692334651947,\n",
       "  0.9538461565971375,\n",
       "  0.947692334651947,\n",
       "  0.9569230675697327,\n",
       "  0.9558974504470825,\n",
       "  0.9610256552696228,\n",
       "  0.9558974504470825,\n",
       "  0.9702563881874084,\n",
       "  0.9743589758872986,\n",
       "  0.964102566242218,\n",
       "  0.9610256552696228,\n",
       "  0.9723076820373535,\n",
       "  0.9723076820373535,\n",
       "  0.9723076820373535,\n",
       "  0.9702563881874084,\n",
       "  0.9733333587646484,\n",
       "  0.9671794772148132,\n",
       "  0.9743589758872986,\n",
       "  0.9753845930099487,\n",
       "  0.9764102697372437,\n",
       "  0.9733333587646484,\n",
       "  0.9774358868598938,\n",
       "  0.9774358868598938,\n",
       "  0.9712820649147034,\n",
       "  0.9774358868598938,\n",
       "  0.9661538600921631,\n",
       "  0.9774358868598938,\n",
       "  0.9753845930099487,\n",
       "  0.9774358868598938,\n",
       "  0.9774358868598938,\n",
       "  0.9743589758872986,\n",
       "  0.9764102697372437,\n",
       "  0.9764102697372437,\n",
       "  0.9774358868598938,\n",
       "  0.9743589758872986,\n",
       "  0.9753845930099487,\n",
       "  0.9764102697372437,\n",
       "  0.9774358868598938,\n",
       "  0.9702563881874084,\n",
       "  0.9764102697372437,\n",
       "  0.9712820649147034,\n",
       "  0.980512797832489,\n",
       "  0.9743589758872986,\n",
       "  0.9723076820373535,\n",
       "  0.9610256552696228,\n",
       "  0.9753845930099487,\n",
       "  0.980512797832489,\n",
       "  0.9774358868598938,\n",
       "  0.9712820649147034,\n",
       "  0.9753845930099487,\n",
       "  0.9712820649147034,\n",
       "  0.9764102697372437,\n",
       "  0.9784615635871887,\n",
       "  0.980512797832489,\n",
       "  0.9784615635871887,\n",
       "  0.9733333587646484,\n",
       "  0.980512797832489,\n",
       "  0.9815384745597839,\n",
       "  0.9784615635871887,\n",
       "  0.9794871807098389,\n",
       "  0.9774358868598938,\n",
       "  0.9764102697372437,\n",
       "  0.9794871807098389,\n",
       "  0.9764102697372437,\n",
       "  0.9794871807098389,\n",
       "  0.9753845930099487,\n",
       "  0.9743589758872986,\n",
       "  0.9753845930099487,\n",
       "  0.9753845930099487,\n",
       "  0.9507692456245422,\n",
       "  0.9825640916824341,\n",
       "  0.9774358868598938,\n",
       "  0.9764102697372437,\n",
       "  0.9784615635871887,\n",
       "  0.9671794772148132,\n",
       "  0.9671794772148132,\n",
       "  0.9753845930099487,\n",
       "  0.9723076820373535,\n",
       "  0.9733333587646484,\n",
       "  0.980512797832489,\n",
       "  0.9784615635871887,\n",
       "  0.9794871807098389,\n",
       "  0.9815384745597839,\n",
       "  0.9784615635871887,\n",
       "  0.9774358868598938]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7095ead5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b9d97d8a00>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "# val_loss가 더 크면 과적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94070359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b9d97f5490>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72c4b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5207311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39a421fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y_oh)\n",
    "model = Sequential()\n",
    "model.add(Dense(36, activation='relu', input_dim=12))\n",
    "model.add(Dense(18, activation='relu'))\n",
    "model.add(Dense(9, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# model.summary()\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea998c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "430af56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a6d91bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = './deep_model/model_check/{epoch:02d}-{val_loss:4f}.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath = modelpath, monitor='val_loss', verbose=1, \\\n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "573443c0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 2.1517 - accuracy: 0.7305  \n",
      "Epoch 1: val_loss improved from inf to 0.24590, saving model to ./deep_model/model_check\\01-0.245904.hdf5\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 1.8362 - accuracy: 0.7629 - val_loss: 0.2459 - val_accuracy: 0.9272\n",
      "Epoch 2/200\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 0.2222 - accuracy: 0.9306\n",
      "Epoch 2: val_loss improved from 0.24590 to 0.21056, saving model to ./deep_model/model_check\\02-0.210560.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.2210 - accuracy: 0.9297 - val_loss: 0.2106 - val_accuracy: 0.9262\n",
      "Epoch 3/200\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.2071 - accuracy: 0.9326\n",
      "Epoch 3: val_loss improved from 0.21056 to 0.20342, saving model to ./deep_model/model_check\\03-0.203420.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.2075 - accuracy: 0.9315 - val_loss: 0.2034 - val_accuracy: 0.9303\n",
      "Epoch 4/200\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.2027 - accuracy: 0.9330\n",
      "Epoch 4: val_loss did not improve from 0.20342\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.2024 - accuracy: 0.9335 - val_loss: 0.2040 - val_accuracy: 0.9313\n",
      "Epoch 5/200\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.1914 - accuracy: 0.9355\n",
      "Epoch 5: val_loss improved from 0.20342 to 0.19716, saving model to ./deep_model/model_check\\05-0.197159.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1978 - accuracy: 0.9335 - val_loss: 0.1972 - val_accuracy: 0.9333\n",
      "Epoch 6/200\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.1948 - accuracy: 0.9356\n",
      "Epoch 6: val_loss improved from 0.19716 to 0.19486, saving model to ./deep_model/model_check\\06-0.194864.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1947 - accuracy: 0.9351 - val_loss: 0.1949 - val_accuracy: 0.9344\n",
      "Epoch 7/200\n",
      "63/78 [=======================>......] - ETA: 0s - loss: 0.1838 - accuracy: 0.9403\n",
      "Epoch 7: val_loss improved from 0.19486 to 0.19007, saving model to ./deep_model/model_check\\07-0.190066.hdf5\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.1910 - accuracy: 0.9369 - val_loss: 0.1901 - val_accuracy: 0.9333\n",
      "Epoch 8/200\n",
      "56/78 [====================>.........] - ETA: 0s - loss: 0.1915 - accuracy: 0.9368\n",
      "Epoch 8: val_loss did not improve from 0.19007\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1884 - accuracy: 0.9369 - val_loss: 0.1943 - val_accuracy: 0.9354\n",
      "Epoch 9/200\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.1842 - accuracy: 0.9397\n",
      "Epoch 9: val_loss improved from 0.19007 to 0.17902, saving model to ./deep_model/model_check\\09-0.179024.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1840 - accuracy: 0.9389 - val_loss: 0.1790 - val_accuracy: 0.9344\n",
      "Epoch 10/200\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.1786 - accuracy: 0.9411\n",
      "Epoch 10: val_loss improved from 0.17902 to 0.17657, saving model to ./deep_model/model_check\\10-0.176570.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1794 - accuracy: 0.9407 - val_loss: 0.1766 - val_accuracy: 0.9374\n",
      "Epoch 11/200\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.1741 - accuracy: 0.9422\n",
      "Epoch 11: val_loss did not improve from 0.17657\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1755 - accuracy: 0.9410 - val_loss: 0.1782 - val_accuracy: 0.9395\n",
      "Epoch 12/200\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.1688 - accuracy: 0.9452\n",
      "Epoch 12: val_loss improved from 0.17657 to 0.17277, saving model to ./deep_model/model_check\\12-0.172771.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1703 - accuracy: 0.9438 - val_loss: 0.1728 - val_accuracy: 0.9405\n",
      "Epoch 13/200\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.1682 - accuracy: 0.9431\n",
      "Epoch 13: val_loss improved from 0.17277 to 0.16719, saving model to ./deep_model/model_check\\13-0.167193.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1678 - accuracy: 0.9433 - val_loss: 0.1672 - val_accuracy: 0.9446\n",
      "Epoch 14/200\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.1543 - accuracy: 0.9485\n",
      "Epoch 14: val_loss improved from 0.16719 to 0.14982, saving model to ./deep_model/model_check\\14-0.149816.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1580 - accuracy: 0.9461 - val_loss: 0.1498 - val_accuracy: 0.9508\n",
      "Epoch 15/200\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.1456 - accuracy: 0.9524\n",
      "Epoch 15: val_loss improved from 0.14982 to 0.14130, saving model to ./deep_model/model_check\\15-0.141298.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1477 - accuracy: 0.9500 - val_loss: 0.1413 - val_accuracy: 0.9528\n",
      "Epoch 16/200\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.1453 - accuracy: 0.9499\n",
      "Epoch 16: val_loss improved from 0.14130 to 0.12725, saving model to ./deep_model/model_check\\16-0.127251.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1426 - accuracy: 0.9500 - val_loss: 0.1273 - val_accuracy: 0.9528\n",
      "Epoch 17/200\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.1371 - accuracy: 0.9507\n",
      "Epoch 17: val_loss improved from 0.12725 to 0.12202, saving model to ./deep_model/model_check\\17-0.122024.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1367 - accuracy: 0.9520 - val_loss: 0.1220 - val_accuracy: 0.9518\n",
      "Epoch 18/200\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.1413 - accuracy: 0.9518\n",
      "Epoch 18: val_loss improved from 0.12202 to 0.11596, saving model to ./deep_model/model_check\\18-0.115957.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9530 - val_loss: 0.1160 - val_accuracy: 0.9590\n",
      "Epoch 19/200\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.1312 - accuracy: 0.9530\n",
      "Epoch 19: val_loss did not improve from 0.11596\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9551 - val_loss: 0.1211 - val_accuracy: 0.9497\n",
      "Epoch 20/200\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.1205 - accuracy: 0.9558\n",
      "Epoch 20: val_loss improved from 0.11596 to 0.10795, saving model to ./deep_model/model_check\\20-0.107947.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1257 - accuracy: 0.9530 - val_loss: 0.1079 - val_accuracy: 0.9672\n",
      "Epoch 21/200\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.1169 - accuracy: 0.9597\n",
      "Epoch 21: val_loss improved from 0.10795 to 0.10431, saving model to ./deep_model/model_check\\21-0.104313.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9595 - val_loss: 0.1043 - val_accuracy: 0.9528\n",
      "Epoch 22/200\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.1187 - accuracy: 0.9597\n",
      "Epoch 22: val_loss improved from 0.10431 to 0.10264, saving model to ./deep_model/model_check\\22-0.102635.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1203 - accuracy: 0.9592 - val_loss: 0.1026 - val_accuracy: 0.9713\n",
      "Epoch 23/200\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.1149 - accuracy: 0.9621\n",
      "Epoch 23: val_loss improved from 0.10264 to 0.09386, saving model to ./deep_model/model_check\\23-0.093858.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1140 - accuracy: 0.9625 - val_loss: 0.0939 - val_accuracy: 0.9579\n",
      "Epoch 24/200\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.1138 - accuracy: 0.9629\n",
      "Epoch 24: val_loss improved from 0.09386 to 0.09023, saving model to ./deep_model/model_check\\24-0.090228.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1086 - accuracy: 0.9651 - val_loss: 0.0902 - val_accuracy: 0.9692\n",
      "Epoch 25/200\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.1063 - accuracy: 0.9662\n",
      "Epoch 25: val_loss improved from 0.09023 to 0.08773, saving model to ./deep_model/model_check\\25-0.087729.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1064 - accuracy: 0.9666 - val_loss: 0.0877 - val_accuracy: 0.9703\n",
      "Epoch 26/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/78 [========================>.....] - ETA: 0s - loss: 0.1060 - accuracy: 0.9658\n",
      "Epoch 26: val_loss improved from 0.08773 to 0.08256, saving model to ./deep_model/model_check\\26-0.082563.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9659 - val_loss: 0.0826 - val_accuracy: 0.9723\n",
      "Epoch 27/200\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.1006 - accuracy: 0.9685\n",
      "Epoch 27: val_loss improved from 0.08256 to 0.07985, saving model to ./deep_model/model_check\\27-0.079852.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9687 - val_loss: 0.0799 - val_accuracy: 0.9651\n",
      "Epoch 28/200\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0965 - accuracy: 0.9693\n",
      "Epoch 28: val_loss improved from 0.07985 to 0.07656, saving model to ./deep_model/model_check\\28-0.076556.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.9697 - val_loss: 0.0766 - val_accuracy: 0.9692\n",
      "Epoch 29/200\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0958 - accuracy: 0.9675\n",
      "Epoch 29: val_loss did not improve from 0.07656\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9672 - val_loss: 0.0975 - val_accuracy: 0.9559\n",
      "Epoch 30/200\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.1018 - accuracy: 0.9680\n",
      "Epoch 30: val_loss did not improve from 0.07656\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1023 - accuracy: 0.9684 - val_loss: 0.0819 - val_accuracy: 0.9662\n",
      "Epoch 31/200\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0894 - accuracy: 0.9723\n",
      "Epoch 31: val_loss improved from 0.07656 to 0.07224, saving model to ./deep_model/model_check\\31-0.072244.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.9705 - val_loss: 0.0722 - val_accuracy: 0.9703\n",
      "Epoch 32/200\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0897 - accuracy: 0.9735\n",
      "Epoch 32: val_loss did not improve from 0.07224\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0882 - accuracy: 0.9731 - val_loss: 0.0802 - val_accuracy: 0.9774\n",
      "Epoch 33/200\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0853 - accuracy: 0.9737\n",
      "Epoch 33: val_loss improved from 0.07224 to 0.06682, saving model to ./deep_model/model_check\\33-0.066823.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0867 - accuracy: 0.9720 - val_loss: 0.0668 - val_accuracy: 0.9703\n",
      "Epoch 34/200\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.1001 - accuracy: 0.9643\n",
      "Epoch 34: val_loss improved from 0.06682 to 0.06496, saving model to ./deep_model/model_check\\34-0.064959.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0986 - accuracy: 0.9661 - val_loss: 0.0650 - val_accuracy: 0.9733\n",
      "Epoch 35/200\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0883 - accuracy: 0.9718\n",
      "Epoch 35: val_loss improved from 0.06496 to 0.06383, saving model to ./deep_model/model_check\\35-0.063827.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0914 - accuracy: 0.9720 - val_loss: 0.0638 - val_accuracy: 0.9744\n",
      "Epoch 36/200\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0853 - accuracy: 0.9742\n",
      "Epoch 36: val_loss improved from 0.06383 to 0.05994, saving model to ./deep_model/model_check\\36-0.059936.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.9751 - val_loss: 0.0599 - val_accuracy: 0.9754\n",
      "Epoch 37/200\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0854 - accuracy: 0.9735\n",
      "Epoch 37: val_loss improved from 0.05994 to 0.05833, saving model to ./deep_model/model_check\\37-0.058326.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0844 - accuracy: 0.9736 - val_loss: 0.0583 - val_accuracy: 0.9744\n",
      "Epoch 38/200\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0811 - accuracy: 0.9751\n",
      "Epoch 38: val_loss improved from 0.05833 to 0.05467, saving model to ./deep_model/model_check\\38-0.054669.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9754 - val_loss: 0.0547 - val_accuracy: 0.9774\n",
      "Epoch 39/200\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0807 - accuracy: 0.9767\n",
      "Epoch 39: val_loss did not improve from 0.05467\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.9756 - val_loss: 0.0701 - val_accuracy: 0.9785\n",
      "Epoch 40/200\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0782 - accuracy: 0.9748\n",
      "Epoch 40: val_loss improved from 0.05467 to 0.05282, saving model to ./deep_model/model_check\\40-0.052817.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0766 - accuracy: 0.9754 - val_loss: 0.0528 - val_accuracy: 0.9805\n",
      "Epoch 41/200\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0813 - accuracy: 0.9749\n",
      "Epoch 41: val_loss did not improve from 0.05282\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9764 - val_loss: 0.0538 - val_accuracy: 0.9774\n",
      "Epoch 42/200\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0785 - accuracy: 0.9759\n",
      "Epoch 42: val_loss did not improve from 0.05282\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.9764 - val_loss: 0.0538 - val_accuracy: 0.9815\n",
      "Epoch 43/200\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0735 - accuracy: 0.9772\n",
      "Epoch 43: val_loss did not improve from 0.05282\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.9772 - val_loss: 0.0575 - val_accuracy: 0.9815\n",
      "Epoch 44/200\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0786 - accuracy: 0.9761\n",
      "Epoch 44: val_loss did not improve from 0.05282\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.9774 - val_loss: 0.0567 - val_accuracy: 0.9815\n",
      "Epoch 45/200\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0730 - accuracy: 0.9769\n",
      "Epoch 45: val_loss did not improve from 0.05282\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.9779 - val_loss: 0.0558 - val_accuracy: 0.9795\n",
      "Epoch 46/200\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0784 - accuracy: 0.9737\n",
      "Epoch 46: val_loss did not improve from 0.05282\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.9731 - val_loss: 0.0675 - val_accuracy: 0.9795\n",
      "Epoch 47/200\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0777 - accuracy: 0.9778\n",
      "Epoch 47: val_loss improved from 0.05282 to 0.04795, saving model to ./deep_model/model_check\\47-0.047948.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0768 - accuracy: 0.9774 - val_loss: 0.0479 - val_accuracy: 0.9785\n",
      "Epoch 48/200\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0770 - accuracy: 0.9754\n",
      "Epoch 48: val_loss improved from 0.04795 to 0.04735, saving model to ./deep_model/model_check\\48-0.047347.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.9764 - val_loss: 0.0473 - val_accuracy: 0.9795\n",
      "Epoch 49/200\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0706 - accuracy: 0.9791\n",
      "Epoch 49: val_loss did not improve from 0.04735\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0706 - accuracy: 0.9792 - val_loss: 0.0496 - val_accuracy: 0.9815\n",
      "Epoch 50/200\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0803 - accuracy: 0.9764\n",
      "Epoch 50: val_loss improved from 0.04735 to 0.04534, saving model to ./deep_model/model_check\\50-0.045337.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.9766 - val_loss: 0.0453 - val_accuracy: 0.9815\n",
      "Epoch 51/200\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0772 - accuracy: 0.9776\n",
      "Epoch 51: val_loss did not improve from 0.04534\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.9787 - val_loss: 0.0680 - val_accuracy: 0.9754\n",
      "Epoch 52/200\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0730 - accuracy: 0.9768\n",
      "Epoch 52: val_loss did not improve from 0.04534\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0711 - accuracy: 0.9774 - val_loss: 0.0490 - val_accuracy: 0.9836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/200\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0710 - accuracy: 0.9792\n",
      "Epoch 53: val_loss did not improve from 0.04534\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0728 - accuracy: 0.9779 - val_loss: 0.0477 - val_accuracy: 0.9815\n",
      "Epoch 54/200\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0658 - accuracy: 0.9811\n",
      "Epoch 54: val_loss improved from 0.04534 to 0.04411, saving model to ./deep_model/model_check\\54-0.044111.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0673 - accuracy: 0.9808 - val_loss: 0.0441 - val_accuracy: 0.9815\n",
      "Epoch 55/200\n",
      "62/78 [======================>.......] - ETA: 0s - loss: 0.0780 - accuracy: 0.9755\n",
      "Epoch 55: val_loss did not improve from 0.04411\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.9766 - val_loss: 0.0449 - val_accuracy: 0.9846\n",
      "Epoch 56/200\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0631 - accuracy: 0.9806\n",
      "Epoch 56: val_loss did not improve from 0.04411\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.9797 - val_loss: 0.0593 - val_accuracy: 0.9805\n",
      "Epoch 57/200\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0721 - accuracy: 0.9779\n",
      "Epoch 57: val_loss did not improve from 0.04411\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.9790 - val_loss: 0.0500 - val_accuracy: 0.9826\n",
      "Epoch 58/200\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0695 - accuracy: 0.9815\n",
      "Epoch 58: val_loss improved from 0.04411 to 0.04154, saving model to ./deep_model/model_check\\58-0.041542.hdf5\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.9790 - val_loss: 0.0415 - val_accuracy: 0.9815\n",
      "Epoch 59/200\n",
      "63/78 [=======================>......] - ETA: 0s - loss: 0.0707 - accuracy: 0.9775\n",
      "Epoch 59: val_loss did not improve from 0.04154\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.9784 - val_loss: 0.0462 - val_accuracy: 0.9836\n",
      "Epoch 60/200\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0671 - accuracy: 0.9768\n",
      "Epoch 60: val_loss did not improve from 0.04154\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0693 - accuracy: 0.9772 - val_loss: 0.0456 - val_accuracy: 0.9826\n",
      "Epoch 61/200\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0593 - accuracy: 0.9820\n",
      "Epoch 61: val_loss did not improve from 0.04154\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0639 - accuracy: 0.9808 - val_loss: 0.0505 - val_accuracy: 0.9856\n",
      "Epoch 62/200\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0695 - accuracy: 0.9783\n",
      "Epoch 62: val_loss did not improve from 0.04154\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.9795 - val_loss: 0.0445 - val_accuracy: 0.9836\n",
      "Epoch 63/200\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0748 - accuracy: 0.9783\n",
      "Epoch 63: val_loss did not improve from 0.04154\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.9790 - val_loss: 0.0461 - val_accuracy: 0.9846\n",
      "Epoch 64/200\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0747 - accuracy: 0.9751\n",
      "Epoch 64: val_loss did not improve from 0.04154\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.9746 - val_loss: 0.0696 - val_accuracy: 0.9836\n",
      "Epoch 65/200\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0690 - accuracy: 0.9823\n",
      "Epoch 65: val_loss did not improve from 0.04154\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.9823 - val_loss: 0.0457 - val_accuracy: 0.9826\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, validation_split=0.2, batch_size=50, epochs=200, \\\n",
    "                    callbacks=[checkpointer, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed885e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.8362171649932861,\n",
       "  0.22101645171642303,\n",
       "  0.20752593874931335,\n",
       "  0.2023998498916626,\n",
       "  0.1978389322757721,\n",
       "  0.19467288255691528,\n",
       "  0.1910099983215332,\n",
       "  0.18839509785175323,\n",
       "  0.18396711349487305,\n",
       "  0.17935608327388763,\n",
       "  0.17553885281085968,\n",
       "  0.17033815383911133,\n",
       "  0.16783657670021057,\n",
       "  0.15798652172088623,\n",
       "  0.14773820340633392,\n",
       "  0.14257194101810455,\n",
       "  0.13673028349876404,\n",
       "  0.13472716510295868,\n",
       "  0.1261930763721466,\n",
       "  0.12566296756267548,\n",
       "  0.11913930624723434,\n",
       "  0.1202542707324028,\n",
       "  0.11402010172605515,\n",
       "  0.10858510434627533,\n",
       "  0.10635822266340256,\n",
       "  0.10650238394737244,\n",
       "  0.10079745203256607,\n",
       "  0.09750794619321823,\n",
       "  0.09995628148317337,\n",
       "  0.1023382619023323,\n",
       "  0.09403134882450104,\n",
       "  0.08820734173059464,\n",
       "  0.08672801405191422,\n",
       "  0.09860642999410629,\n",
       "  0.09140116721391678,\n",
       "  0.08269739151000977,\n",
       "  0.08435942977666855,\n",
       "  0.08136594295501709,\n",
       "  0.08132354170084,\n",
       "  0.07658106833696365,\n",
       "  0.07693367451429367,\n",
       "  0.07528558373451233,\n",
       "  0.07514926791191101,\n",
       "  0.07539953291416168,\n",
       "  0.07173418253660202,\n",
       "  0.08065810054540634,\n",
       "  0.07681728154420853,\n",
       "  0.07374922186136246,\n",
       "  0.07063501328229904,\n",
       "  0.08082260936498642,\n",
       "  0.07162199169397354,\n",
       "  0.0711420550942421,\n",
       "  0.07284033298492432,\n",
       "  0.06730198860168457,\n",
       "  0.07151961326599121,\n",
       "  0.06829345226287842,\n",
       "  0.06674963980913162,\n",
       "  0.07180764526128769,\n",
       "  0.06863883882761002,\n",
       "  0.06931740790605545,\n",
       "  0.06386378407478333,\n",
       "  0.06559093296527863,\n",
       "  0.07159344106912613,\n",
       "  0.07644012570381165,\n",
       "  0.06760778278112411],\n",
       " 'accuracy': [0.7628945112228394,\n",
       "  0.9296895265579224,\n",
       "  0.9314857721328735,\n",
       "  0.9335386157035828,\n",
       "  0.9335386157035828,\n",
       "  0.9350782632827759,\n",
       "  0.936874508857727,\n",
       "  0.936874508857727,\n",
       "  0.9389273524284363,\n",
       "  0.9407236576080322,\n",
       "  0.9409802556037903,\n",
       "  0.9438029527664185,\n",
       "  0.9432896971702576,\n",
       "  0.9461123943328857,\n",
       "  0.9499614834785461,\n",
       "  0.9499614834785461,\n",
       "  0.9520143866539001,\n",
       "  0.9530407786369324,\n",
       "  0.9550936818122864,\n",
       "  0.9530407786369324,\n",
       "  0.9594559669494629,\n",
       "  0.9591993689537048,\n",
       "  0.9625352621078491,\n",
       "  0.9651013612747192,\n",
       "  0.9666410088539124,\n",
       "  0.9658711552619934,\n",
       "  0.9686938524246216,\n",
       "  0.9697203040122986,\n",
       "  0.9671542048454285,\n",
       "  0.9684372544288635,\n",
       "  0.9704900979995728,\n",
       "  0.9730561971664429,\n",
       "  0.9720297455787659,\n",
       "  0.9661278128623962,\n",
       "  0.9720297455787659,\n",
       "  0.9751090407371521,\n",
       "  0.973569393157959,\n",
       "  0.9753656387329102,\n",
       "  0.975622296333313,\n",
       "  0.9753656387329102,\n",
       "  0.9763920903205872,\n",
       "  0.9763920903205872,\n",
       "  0.9771619439125061,\n",
       "  0.9774185419082642,\n",
       "  0.9779317378997803,\n",
       "  0.9730561971664429,\n",
       "  0.9774185419082642,\n",
       "  0.9763920903205872,\n",
       "  0.9792147874832153,\n",
       "  0.9766486883163452,\n",
       "  0.9787015914916992,\n",
       "  0.9774185419082642,\n",
       "  0.9779317378997803,\n",
       "  0.9807544350624084,\n",
       "  0.9766486883163452,\n",
       "  0.9797279834747314,\n",
       "  0.9789581894874573,\n",
       "  0.9789581894874573,\n",
       "  0.9784449338912964,\n",
       "  0.9771619439125061,\n",
       "  0.9807544350624084,\n",
       "  0.9794713854789734,\n",
       "  0.9789581894874573,\n",
       "  0.974595844745636,\n",
       "  0.9822940826416016],\n",
       " 'val_loss': [0.24590352177619934,\n",
       "  0.21055975556373596,\n",
       "  0.20341981947422028,\n",
       "  0.20398764312267303,\n",
       "  0.19715875387191772,\n",
       "  0.19486366212368011,\n",
       "  0.19006551802158356,\n",
       "  0.19431203603744507,\n",
       "  0.17902421951293945,\n",
       "  0.17657049000263214,\n",
       "  0.1782168745994568,\n",
       "  0.17277127504348755,\n",
       "  0.16719277203083038,\n",
       "  0.14981567859649658,\n",
       "  0.14129780232906342,\n",
       "  0.12725089490413666,\n",
       "  0.12202446907758713,\n",
       "  0.11595694720745087,\n",
       "  0.12106940895318985,\n",
       "  0.10794702172279358,\n",
       "  0.10431338846683502,\n",
       "  0.10263548046350479,\n",
       "  0.09385839104652405,\n",
       "  0.09022776037454605,\n",
       "  0.08772866427898407,\n",
       "  0.08256250619888306,\n",
       "  0.07985171675682068,\n",
       "  0.07655584067106247,\n",
       "  0.09754644334316254,\n",
       "  0.08191952109336853,\n",
       "  0.07224419713020325,\n",
       "  0.08023440092802048,\n",
       "  0.0668228417634964,\n",
       "  0.0649585947394371,\n",
       "  0.06382692605257034,\n",
       "  0.0599357895553112,\n",
       "  0.05832623317837715,\n",
       "  0.05466906726360321,\n",
       "  0.07012347131967545,\n",
       "  0.05281650647521019,\n",
       "  0.05384184047579765,\n",
       "  0.05375943332910538,\n",
       "  0.05746317654848099,\n",
       "  0.056690819561481476,\n",
       "  0.05582823604345322,\n",
       "  0.06745357066392899,\n",
       "  0.04794765263795853,\n",
       "  0.047346606850624084,\n",
       "  0.04960447549819946,\n",
       "  0.04533686488866806,\n",
       "  0.0680287703871727,\n",
       "  0.04901910573244095,\n",
       "  0.047715116292238235,\n",
       "  0.04411132261157036,\n",
       "  0.044947363436222076,\n",
       "  0.05929688364267349,\n",
       "  0.050035636872053146,\n",
       "  0.04154159501194954,\n",
       "  0.04615599662065506,\n",
       "  0.04562204331159592,\n",
       "  0.050481028854846954,\n",
       "  0.04449254274368286,\n",
       "  0.0461115762591362,\n",
       "  0.06962580978870392,\n",
       "  0.04571588709950447],\n",
       " 'val_accuracy': [0.9271795153617859,\n",
       "  0.926153838634491,\n",
       "  0.9302564263343811,\n",
       "  0.9312820434570312,\n",
       "  0.9333333373069763,\n",
       "  0.9343589544296265,\n",
       "  0.9333333373069763,\n",
       "  0.9353846311569214,\n",
       "  0.9343589544296265,\n",
       "  0.9374359250068665,\n",
       "  0.9394871592521667,\n",
       "  0.9405128359794617,\n",
       "  0.944615364074707,\n",
       "  0.9507692456245422,\n",
       "  0.9528205394744873,\n",
       "  0.9528205394744873,\n",
       "  0.9517948627471924,\n",
       "  0.9589743614196777,\n",
       "  0.9497435688972473,\n",
       "  0.9671794772148132,\n",
       "  0.9528205394744873,\n",
       "  0.9712820649147034,\n",
       "  0.9579487442970276,\n",
       "  0.9692307710647583,\n",
       "  0.9702563881874084,\n",
       "  0.9723076820373535,\n",
       "  0.9651281833648682,\n",
       "  0.9692307710647583,\n",
       "  0.9558974504470825,\n",
       "  0.9661538600921631,\n",
       "  0.9702563881874084,\n",
       "  0.9774358868598938,\n",
       "  0.9702563881874084,\n",
       "  0.9733333587646484,\n",
       "  0.9743589758872986,\n",
       "  0.9753845930099487,\n",
       "  0.9743589758872986,\n",
       "  0.9774358868598938,\n",
       "  0.9784615635871887,\n",
       "  0.980512797832489,\n",
       "  0.9774358868598938,\n",
       "  0.9815384745597839,\n",
       "  0.9815384745597839,\n",
       "  0.9815384745597839,\n",
       "  0.9794871807098389,\n",
       "  0.9794871807098389,\n",
       "  0.9784615635871887,\n",
       "  0.9794871807098389,\n",
       "  0.9815384745597839,\n",
       "  0.9815384745597839,\n",
       "  0.9753845930099487,\n",
       "  0.983589768409729,\n",
       "  0.9815384745597839,\n",
       "  0.9815384745597839,\n",
       "  0.9846153855323792,\n",
       "  0.980512797832489,\n",
       "  0.9825640916824341,\n",
       "  0.9815384745597839,\n",
       "  0.983589768409729,\n",
       "  0.9825640916824341,\n",
       "  0.9856410026550293,\n",
       "  0.983589768409729,\n",
       "  0.9846153855323792,\n",
       "  0.983589768409729,\n",
       "  0.9825640916824341]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3639bb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b9dc2232b0>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cdf16a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b9dd2dd520>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b7b853b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 0s 677us/step - loss: 0.0458 - accuracy: 0.9858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04577993229031563, 0.9858461618423462]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f4c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
